{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "gpuType": "T4",
      "history_visible": true,
      "mount_file_id": "1g_QEs-Q3vHt8rpo-23EOIpo3TTs7iuUc",
      "authorship_tag": "ABX9TyNSLZm2ASQA1Q9TZCX+6Upu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SVNitinSV/Inventory_projection/blob/main/nlp_Tek_clover_IP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JR4DCcki1nlk"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path=\"/content/drive/MyDrive/ML Data - Outbound - updated.xlsx - Sheet1.csv\"\n",
        "df = pd.read_csv(path)\n",
        "df.head(5)"
      ],
      "metadata": {
        "id": "UR2lESBA7RWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "sqqTGu-m8WBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Original DataFrame:\")\n",
        "print(df.head())\n",
        "\n",
        "# Keep only the specified columns\n",
        "columns_to_keep = ['Item code', 'Item Description', 'Transaction Date', 'Qty']\n",
        "df = df[columns_to_keep]\n",
        "\n",
        "# Display the DataFrame after dropping columns\n",
        "print(\"\\nDataFrame after dropping unnecessary columns:\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "xeasRDrZxmrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(5)"
      ],
      "metadata": {
        "id": "7UNksRHLxprP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp_df = df[['Item Description', 'Item code']]\n",
        "nlp_df['Item Description'] = nlp_df['Item Description'].astype(str)"
      ],
      "metadata": {
        "id": "QeHWbhzVxrdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp_df.head(5)"
      ],
      "metadata": {
        "id": "tYju1jopUL9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "TDOeFIOOUO_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "i93ZWq7eVr5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    # Remove numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove punctuation and special symbols\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    # Lemmatize the tokens\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "    # Join tokens back to string\n",
        "    processed_text = ' '.join(tokens)\n",
        "    return processed_text"
      ],
      "metadata": {
        "id": "lmJVuX37VveM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp_df['Item_Description_processed'] = nlp_df['Item Description'].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "81r4fBmgV67m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp_df.sample(5)"
      ],
      "metadata": {
        "id": "BEhLhrxBWCJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wordcloud"
      ],
      "metadata": {
        "id": "mattowICXrP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "67zM-iEgborA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = ' '.join(nlp_df['Item_Description_processed'])"
      ],
      "metadata": {
        "id": "6jZkCNaWb5Wy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "64hKIG1kctW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wc = WordCloud(background_color='white',width=1920, height=1080).generate(text)\n",
        "plt.imshow(wc)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "M2vHbghfcvix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob"
      ],
      "metadata": {
        "id": "DRUxrrQdeM7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "from nltk.cluster import KMeansClusterer\n",
        "import nltk\n",
        "from sklearn import cluster\n",
        "from sklearn import metrics\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "from sklearn.metrics import silhouette_samples, silhouette_score"
      ],
      "metadata": {
        "id": "llXyM9rjeWrf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_list = nlp_df['Item_Description_processed'].tolist()\n",
        "text_list = [nltk.word_tokenize(sentence.lower()) for sentence in text_list]\n"
      ],
      "metadata": {
        "id": "adlddnTY7J8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text_list[0])"
      ],
      "metadata": {
        "id": "ANmxs9Yc7YQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install -U gensim"
      ],
      "metadata": {
        "id": "fkbk_UVk0f2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_hub as hub\n",
        "\n",
        "embed = hub.load(\"https://www.kaggle.com/models/google/wiki-words/TensorFlow2/250/1\")"
      ],
      "metadata": {
        "id": "qpQJgMnu2J9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Word2Vec(text_list, vector_size=100, workers=1, seed=42)"
      ],
      "metadata": {
        "id": "WYh7yWx3loc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get word embeddings\n",
        "word_embeddings = model.wv"
      ],
      "metadata": {
        "id": "DB0lmMX5u_QN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(word_embeddings)"
      ],
      "metadata": {
        "id": "wmZpkAimvg7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate over the vocabulary and print embeddings for each word\n",
        "\n",
        "model.wv.most_similar(\"spray\")"
      ],
      "metadata": {
        "id": "RJ3SWMbS6nrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize(list_of_docs, model):\n",
        "    \"\"\"Generate vectors for list of documents using a Word Embedding\n",
        "\n",
        "    Args:\n",
        "        list_of_docs: List of documents\n",
        "        model: Gensim's Word Embedding\n",
        "\n",
        "    Returns:\n",
        "        List of document vectors\n",
        "    \"\"\"\n",
        "    features = []\n",
        "\n",
        "    for tokens in list_of_docs:\n",
        "        zero_vector = np.zeros(model.vector_size)\n",
        "        vectors = []\n",
        "        for token in tokens:\n",
        "            if token in model.wv:\n",
        "                try:\n",
        "                    vectors.append(model.wv[token])\n",
        "                except KeyError:\n",
        "                    continue\n",
        "        if vectors:\n",
        "            vectors = np.asarray(vectors)\n",
        "            avg_vec = vectors.mean(axis=0)\n",
        "            features.append(avg_vec)\n",
        "        else:\n",
        "            features.append(zero_vector)\n",
        "    return features\n",
        "\n",
        "vectorized_docs = vectorize(text_list, model=model)\n",
        "len(vectorized_docs), len(vectorized_docs[0])"
      ],
      "metadata": {
        "id": "5QEYhnv7-iUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mbkmeans_clusters(\n",
        "\tX,\n",
        "    k,\n",
        "    mb,\n",
        "    print_silhouette_values,\n",
        "):\n",
        "    \"\"\"Generate clusters and print Silhouette metrics using MBKmeans\n",
        "\n",
        "    Args:\n",
        "        X: Matrix of features.\n",
        "        k: Number of clusters.\n",
        "        mb: Size of mini-batches.\n",
        "        print_silhouette_values: Print silhouette values per cluster.\n",
        "\n",
        "    Returns:\n",
        "        Trained clustering model and labels based on X.\n",
        "    \"\"\"\n",
        "    km = MiniBatchKMeans(n_clusters=k, batch_size=mb).fit(X)\n",
        "    print(f\"For n_clusters = {k}\")\n",
        "    print(f\"Silhouette coefficient: {silhouette_score(X, km.labels_):0.2f}\")\n",
        "    print(f\"Inertia:{km.inertia_}\")\n",
        "\n",
        "    if print_silhouette_values:\n",
        "        sample_silhouette_values = silhouette_samples(X, km.labels_)\n",
        "        print(f\"Silhouette values:\")\n",
        "        silhouette_values = []\n",
        "        for i in range(k):\n",
        "          if not vectorized_docs or np.all(vectorized_docs == 0):\n",
        "            raise ValueError(\"The vectorized_docs matrix is empty or contains all zeros.\")\n",
        "          cluster_silhouette_values = sample_silhouette_values[km.labels_ == i]\n",
        "          silhouette_values.append(\n",
        "                (\n",
        "                    i,\n",
        "                    cluster_silhouette_values.shape[0],\n",
        "                    cluster_silhouette_values.mean(),\n",
        "                    cluster_silhouette_values.min(),\n",
        "                    cluster_silhouette_values.max(),\n",
        "                )\n",
        "            )\n",
        "        silhouette_values = sorted(\n",
        "            silhouette_values, key=lambda tup: tup[2], reverse=True\n",
        "        )\n",
        "        for s in silhouette_values:\n",
        "            print(\n",
        "                f\"    Cluster {s[0]}: Size:{s[1]} | Avg:{s[2]:.2f} | Min:{s[3]:.2f} | Max: {s[4]:.2f}\"\n",
        "            )\n",
        "    return km, km.labels_"
      ],
      "metadata": {
        "id": "PhxFaDXA-4xQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = nlp_df['Item_Description_processed'].values"
      ],
      "metadata": {
        "id": "wShTAUkiBXyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clustering, cluster_labels = mbkmeans_clusters(\n",
        "\tX=vectorized_docs,\n",
        "    k=300,\n",
        "    mb=500,\n",
        "    print_silhouette_values=True,\n",
        ")\n",
        "df_clusters = pd.DataFrame({\n",
        "    \"text\": docs,\n",
        "    \"tokens\": [\" \".join(text) for text in text_list],\n",
        "    \"cluster\": cluster_labels\n",
        "})"
      ],
      "metadata": {
        "id": "7GfhqgSp-_EJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Most representative terms per cluster (based on centroids):\")\n",
        "for i in range(200):\n",
        "    tokens_per_cluster = \"\"\n",
        "    most_representative = model.wv.most_similar(positive=[clustering.cluster_centers_[i]], topn=20)\n",
        "    for t in most_representative:\n",
        "        tokens_per_cluster += f\"{t[0]} \"\n",
        "    print(f\"Cluster {i}: {tokens_per_cluster}\")"
      ],
      "metadata": {
        "id": "6B8zsBc0DWUH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}